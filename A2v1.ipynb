{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score as jaccard\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import scipy.optimize\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from scipy.sparse import lil_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:    \n",
    "        yield json.loads(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(parse(\"data/Video_Games_5.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    299759\n",
       "4.0     93654\n",
       "3.0     49146\n",
       "1.0     30883\n",
       "2.0     24135\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [d for d in data]\n",
    "y = [d['overall'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle data\n",
    "Xy = list(zip(X,y))\n",
    "random.shuffle(Xy)\n",
    "X = np.array([d[0] for d in Xy])\n",
    "y = np.array([d[1] for d in Xy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['len_rev'] = df['reviewText'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall': 5.0,\n",
       " 'verified': True,\n",
       " 'reviewTime': '10 17, 2015',\n",
       " 'reviewerID': 'A1HP7NVNPFMA4N',\n",
       " 'asin': '0700026657',\n",
       " 'reviewerName': 'Ambrosia075',\n",
       " 'reviewText': \"This game is a bit hard to get the hang of, but when you do it's great.\",\n",
       " 'summary': \"but when you do it's great.\",\n",
       " 'unixReviewTime': 1445040000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewerID\n",
       "A0059486XI1Z0P98KP35     5\n",
       "A0220159ZRNBTRKLG08H     6\n",
       "A0266076X6KPZ6CCHGVS    14\n",
       "A0277912HT4JSJKVSL3E    10\n",
       "A02836981FYG9912C66F     7\n",
       "                        ..\n",
       "AZZNK89PXD006            7\n",
       "AZZQCK9ZAKMFR           11\n",
       "AZZT1ERHBSNQ8            7\n",
       "AZZTC2OYVNE2Q            6\n",
       "AZZTOUKVTUMVM            6\n",
       "Length: 55223, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.groupby(['reviewerID']).size()\n",
    "# ['overall'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>style</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [overall, verified, reviewTime, reviewerID, asin, reviewerName, reviewText, summary, unixReviewTime, vote, style, image]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df['reviewerID'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:30000]\n",
    "Xvalid = X[30000:40000]\n",
    "# Xtest = X[40000:50000]\n",
    "\n",
    "ytrain = y[:30000]\n",
    "yvalid = y[30000:40000]\n",
    "# ytest = y[40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [.001,.01, .1, 1, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unigrams, keep punc, tfidf\n",
    "#training data\n",
    "unigrams = defaultdict(int)\n",
    "for d in Xtrain:\n",
    "    #not all data has a review\n",
    "    if 'reviewText' in d:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     unigram = list(ngrams(token, 1))\n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            unigrams[u] += 1\n",
    "\n",
    "#1000 most common from training set\n",
    "mostCommonUni =sorted(unigrams.items(),key=lambda v: v[1],reverse=True)[:1000]\n",
    "unigram_words = [u[0] for u in mostCommonUni]\n",
    "unigramId = dict(zip(unigram_words, range(len(unigram_words))))\n",
    "unigramSet = set(unigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docFreq and tf\n",
    "#training data\n",
    "docFreq = defaultdict(set)\n",
    "for d in Xtrain:\n",
    "    if 'reviewText' in d: \n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            docFreq[u].add(d['reviewerID'])\n",
    "\n",
    "#term freq\n",
    "tf = unigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_uni_punc_tfidf(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    \n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[u])) * tf[u]\n",
    "            feat[unigramId[u]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_1 = [feature_uni_punc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_1 = [feature_uni_punc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, discard punc, tfidf\n",
    "def feature_uni_nopunc_tfidf(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum:\n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[u])) * tf[u]\n",
    "            feat[unigramId[u]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_2 = [feature_uni_nopunc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_2 = [feature_uni_nopunc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, keep punc, counts\n",
    "def feature_uni_punc_wc(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_3 = [feature_uni_punc_wc(d) for d in Xtrain]\n",
    "Xvalid_3 = [feature_uni_punc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams, discard punc, counts\n",
    "def feature_uni_nopunc_wc(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4 = [feature_uni_nopunc_wc(d) for d in Xtrain]\n",
    "Xvalid_4 = [feature_uni_nopunc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start of bigram models\n",
    "bigrams = defaultdict(int)\n",
    "\n",
    "for d in Xtrain:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     bigram = list(ngrams(token, 2)) \n",
    "    if 'reviewText' in d: \n",
    "        text = \" \".join(d['reviewText'].splitlines())\n",
    "        bigram = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "        for b in bigram:\n",
    "            bigrams[b] += 1\n",
    "        \n",
    "#1000 most common from training set\n",
    "mostCommonBi =sorted(bigrams.items(),key=lambda v: v[1],reverse=True)[:1000]\n",
    "bigram_words = [u[0] for u in mostCommonBi]\n",
    "bigramId = dict(zip(bigram_words, range(len(bigram_words))))\n",
    "bigramSet = set(bigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docFreq and tf\n",
    "#training data\n",
    "docFreq = defaultdict(set)\n",
    "for d in Xtrain:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     bigram = list(ngrams(token, 2)) \n",
    "    if 'reviewText' in d: \n",
    "        text = \" \".join(d['reviewText'].splitlines())\n",
    "        bigram = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "        for b in bigram:\n",
    "            docFreq[b].add(d['reviewerID'])\n",
    "\n",
    "#term freq\n",
    "tf = bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, keep punc, tfidf\n",
    "def feature_bi_punc_tfidf(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[b])) * tf[b]\n",
    "            feat[bigramId[b]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_5 = [feature_bi_punc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_5 = [feature_bi_punc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, discard punc, tfidf\n",
    "def feature_bi_nopunc_tfidf(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            tf_idf_word = np.log(len(Xtrain)/ len(docFreq[b])) * tf[b]\n",
    "            feat[bigramId[b]] = tf_idf_word\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_6 = [feature_bi_nopunc_tfidf(d) for d in Xtrain]\n",
    "Xvalid_6 = [feature_bi_nopunc_tfidf(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, keep punc, counts\n",
    "def feature_bi_punc_wc(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            feat[bigramId[b]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_7 = [feature_bi_punc_wc(d) for d in Xtrain]\n",
    "Xvalid_7 = [feature_bi_punc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams, discard punc, counts\n",
    "def feature_bi_nopunc_wc(datum):\n",
    "    feat = [0]*len(bigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     bigram_words = list(ngrams(token, 2))\n",
    "\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "        text = \" \".join(t.splitlines())\n",
    "        bigram_words = [b for b in zip(text.split(\" \")[:-1], text.split(\" \")[1:])]\n",
    "\n",
    "        for b in bigram_words:\n",
    "            if not (b in bigramSet): continue\n",
    "            feat[bigramId[b]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_8 = [feature_bi_nopunc_wc(d) for d in Xtrain]\n",
    "Xvalid_8 = [feature_bi_nopunc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_fit = [Xtrain_1, Xtrain_2, Xtrain_3, Xtrain_4, Xtrain_5, Xtrain_6, Xtrain_7, Xtrain_8]\n",
    "to_pred = [Xvalid_1, Xvalid_2, Xvalid_3, Xvalid_4, Xvalid_5, Xvalid_6, Xvalid_7, Xvalid_8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [3:15:50<3:23:15, 3048.91s/it] /opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      " 62%|██████▎   | 5/8 [5:29:10<3:46:42, 4534.26s/it]/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "100%|██████████| 8/8 [7:57:51<00:00, 3583.98s/it]  \n"
     ]
    }
   ],
   "source": [
    "# pipeline\n",
    "# to_fit = [Xtrain_1, Xtrain_2, Xtrain_3, Xtrain_4, Xtrain_5, Xtrain_6, Xtrain_7, Xtrain_8]\n",
    "# to_pred = [Xvalid_1, Xvalid_2, Xvalid_3, Xvalid_4, Xvalid_5, Xvalid_6, Xvalid_7, Xvalid_8]\n",
    "model_performances = []\n",
    "for i in tqdm(range(len(to_fit))):\n",
    "    for c in C:\n",
    "        clf = LogisticRegression(C = c, fit_intercept=False, max_iter = 100000) \n",
    "        clf.fit(to_fit[i], ytrain)\n",
    "        theta = clf.coef_\n",
    "        predictions = clf.predict(to_pred[i])\n",
    "        correct = predictions == yvalid\n",
    "        acc = sum(correct) / len(correct)\n",
    "        model_performances.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"unigrams, keep punc, tfidf\",\n",
    "\"unigrams, discard punc, tfidf\",\n",
    "\"unigrams, keep punc, counts\",\n",
    "\"unigrams, discard punc, counts\",\n",
    "\"bigrams, keep punc, tfidf\",\n",
    "\"bigrams, discard punc, tfidf\",\n",
    "\"bigrams, keep punc, counts\",\n",
    "\"bigrams, discard punc, counts\"]\n",
    "\n",
    "index_names = []\n",
    "for model in model_names:\n",
    "    for c in C:\n",
    "        index_names.append((model,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(index_names, names=['model','regularization param'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>regularization param</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">unigrams, keep punc, tfidf</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">unigrams, discard punc, tfidf</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">unigrams, keep punc, counts</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">unigrams, discard punc, counts</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bigrams, keep punc, tfidf</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bigrams, discard punc, tfidf</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bigrams, keep punc, counts</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">bigrams, discard punc, counts</th>\n",
       "      <th>0.001</th>\n",
       "      <td>0.6082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.010</th>\n",
       "      <td>0.6145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.100</th>\n",
       "      <td>0.6168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.000</th>\n",
       "      <td>0.6148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.000</th>\n",
       "      <td>0.6139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.000</th>\n",
       "      <td>0.6139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     accuracy\n",
       "model                          regularization param          \n",
       "unigrams, keep punc, tfidf     0.001                   0.6171\n",
       "                               0.010                   0.6167\n",
       "                               0.100                   0.6170\n",
       "                               1.000                   0.6171\n",
       "                               10.000                  0.6168\n",
       "                               100.000                 0.6170\n",
       "unigrams, discard punc, tfidf  0.001                   0.6377\n",
       "                               0.010                   0.6381\n",
       "                               0.100                   0.6380\n",
       "                               1.000                   0.6370\n",
       "                               10.000                  0.6377\n",
       "                               100.000                 0.6375\n",
       "unigrams, keep punc, counts    0.001                   0.6165\n",
       "                               0.010                   0.6322\n",
       "                               0.100                   0.6378\n",
       "                               1.000                   0.6371\n",
       "                               10.000                  0.6366\n",
       "                               100.000                 0.6372\n",
       "unigrams, discard punc, counts 0.001                   0.6211\n",
       "                               0.010                   0.6414\n",
       "                               0.100                   0.6496\n",
       "                               1.000                   0.6477\n",
       "                               10.000                  0.6470\n",
       "                               100.000                 0.6469\n",
       "bigrams, keep punc, tfidf      0.001                   0.6082\n",
       "                               0.010                   0.6122\n",
       "                               0.100                   0.6085\n",
       "                               1.000                   0.6133\n",
       "                               10.000                  0.6076\n",
       "                               100.000                 0.6084\n",
       "bigrams, discard punc, tfidf   0.001                   0.6150\n",
       "                               0.010                   0.6153\n",
       "                               0.100                   0.6155\n",
       "                               1.000                   0.6156\n",
       "                               10.000                  0.6157\n",
       "                               100.000                 0.6150\n",
       "bigrams, keep punc, counts     0.001                   0.6069\n",
       "                               0.010                   0.6122\n",
       "                               0.100                   0.6150\n",
       "                               1.000                   0.6119\n",
       "                               10.000                  0.6103\n",
       "                               100.000                 0.6105\n",
       "bigrams, discard punc, counts  0.001                   0.6082\n",
       "                               0.010                   0.6145\n",
       "                               0.100                   0.6168\n",
       "                               1.000                   0.6148\n",
       "                               10.000                  0.6139\n",
       "                               100.000                 0.6139"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data = model_performances, index = index, columns = ['accuracy'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy    0.6496\n",
       "Name: (unigrams, discard punc, counts, 0.1), dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#accuracy with model trained on subset of data\n",
    "df.sort_values(by = 'accuracy', ascending = False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrain = X[:447819]\n",
    "# Xvalid = X[447819:]\n",
    "# # Xtest = X[40000:50000]\n",
    "\n",
    "# ytrain = y[:447819]\n",
    "# yvalid = y[447819:]\n",
    "# # ytest = y[40000:50000]\n",
    "\n",
    "Xtrain = X[:30000]\n",
    "Xvalid = X[30000:40000]\n",
    "# Xtest = X[40000:50000]\n",
    "\n",
    "ytrain = y[:30000]\n",
    "yvalid = y[30000:40000]\n",
    "# ytest = y[40000:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hypertuning\n",
    "#increase dict size \n",
    "#acc for dict size 5000: 0.6603\n",
    "#acc for dict size 8000: 0.6624\n",
    "#acc for dict size 10000: 0.6621\n",
    "\n",
    "unigrams = defaultdict(int)\n",
    "for d in Xtrain:\n",
    "    #not all data has a review\n",
    "    if 'reviewText' in d:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     unigram = list(ngrams(token, 1))\n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            unigrams[u] += 1\n",
    "\n",
    "#most common from training set\n",
    "mostCommonUni =sorted(unigrams.items(),key=lambda v: v[1],reverse=True)[:8000]\n",
    "unigram_words = [u[0] for u in mostCommonUni]\n",
    "unigramId = dict(zip(unigram_words, range(len(unigram_words))))\n",
    "unigramSet = set(unigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimal model with larger dict size\n",
    "#acc for dict size 5000: 0.6603\n",
    "#acc for dict size 8000: 0.6624\n",
    "#acc for dict size 10000: 0.6621\n",
    "\n",
    "#unigrams, discard punc, counts\n",
    "def feature_uni_nopunc_wc(datum):\n",
    "    feat = [0]*len(unigramSet)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4 = [feature_uni_nopunc_wc(d) for d in Xtrain]\n",
    "Xvalid_4 = [feature_uni_nopunc_wc(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4_ = lil_matrix(Xtrain_4)\n",
    "Xvalid_4_ = lil_matrix(Xvalid_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6621"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C = 0.1, fit_intercept=False, max_iter = 100000) \n",
    "clf.fit(Xtrain_4, ytrain)\n",
    "# theta = clf.coef_\n",
    "predictions = clf.predict(Xvalid_4)\n",
    "correct = predictions == yvalid\n",
    "acc = sum(correct) / len(correct)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:300000]\n",
    "Xvalid = X[300000:400000]\n",
    "\n",
    "ytrain = y[:300000]\n",
    "yvalid = y[300000:400000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = defaultdict(int)\n",
    "for d in Xtrain:\n",
    "    #not all data has a review\n",
    "    if 'reviewText' in d:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     unigram = list(ngrams(token, 1))\n",
    "        t = d['reviewText']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            unigrams[u] += 1\n",
    "\n",
    "#most common from training set\n",
    "mostCommonUni =sorted(unigrams.items(),key=lambda v: v[1],reverse=True)[:8000]\n",
    "unigram_words = [u[0] for u in mostCommonUni]\n",
    "unigramId = dict(zip(unigram_words, range(len(unigram_words))))\n",
    "unigramSet = set(unigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 most common words in 'summary' acc: 0.6519\n",
    "#50 most common: 0.6525\n",
    "unigramsSumm = defaultdict(int)\n",
    "for d in Xtrain:\n",
    "    #not all data has a review\n",
    "    if 'summary' in d:\n",
    "#     token = nltk.word_tokenize(d['text'])\n",
    "#     unigram = list(ngrams(token, 1))\n",
    "        t = d['summary']\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram = text.strip().split()\n",
    "        for u in unigram:\n",
    "            unigramsSumm[u] += 1\n",
    "\n",
    "#50 most common from training set, summary\n",
    "mostCommonUniSumm =sorted(unigramsSumm.items(),key=lambda v: v[1],reverse=True)[:50]\n",
    "unigram_wordsSumm = [u[0] for u in mostCommonUniSumm]\n",
    "#dictsize 5051 (5000 reviews + 50 summary)\n",
    "unigramIdSumm = dict(zip(unigram_wordsSumm, np.arange(8000,8051)))\n",
    "unigramSetSumm = set(unigram_wordsSumm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features including summaries and review length\n",
    "def feature_uni_nopunc_wc_withSumm(datum):\n",
    "    feat = [0]*(len(unigramSet)+len(unigramSetSumm) + 1)\n",
    "    if 'reviewText' in datum: \n",
    "        t = datum['reviewText']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_words = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        for u in unigram_words:\n",
    "            if not (u in unigramSet): continue\n",
    "            feat[unigramId[u]] += 1\n",
    "            \n",
    "    if 'summary' in datum: \n",
    "        t = datum['summary']\n",
    "        t = ''.join([c for c in t.lower() if not c in punctuation])\n",
    "\n",
    "        text = \" \".join(t.splitlines())\n",
    "        unigram_wordsSumm = text.strip().split()\n",
    "    #     token = nltk.word_tokenize(t)\n",
    "    #     unigram_words = list(ngrams(token, 1))\n",
    "        for u in unigram_wordsSumm:\n",
    "            if not (u in unigramSetSumm): continue\n",
    "            feat[unigramIdSumm[u]] += 1\n",
    "        \n",
    "        \n",
    "    if 'reviewText' in datum:\n",
    "        feat[-1] = len(datum['reviewText'])\n",
    "\n",
    "    feat.append(1)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4_ = [feature_uni_nopunc_wc_withSumm(d) for d in Xtrain]\n",
    "Xvalid_4_ = [feature_uni_nopunc_wc_withSumm(d) for d in Xvalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xtrain_4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_4_ = lil_matrix(Xtrain_4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvalid_4_ = lil_matrix(Xvalid_4_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68589"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.68589 dict size 8000, 300000/100000 train/test split\n",
    "clf = LogisticRegression(C = 0.1, fit_intercept=False, max_iter = 50000) \n",
    "clf.fit(Xtrain_4_, ytrain)\n",
    "# theta = clf.coef_\n",
    "predictions = clf.predict(Xvalid_4_)\n",
    "correct = predictions == yvalid\n",
    "acc = sum(correct) / len(correct)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
